{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1. Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.a. Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pandas is data table handling library. you can do roll-up, group-by e.t.c much easir and faster\n",
    "import pandas as pd\n",
    "#re is string processing library. you can find patterns in regular expressions e.t.c\n",
    "import re\n",
    "#os is operating system access library, to navingate to different file paths\n",
    "import os\n",
    "\n",
    "#Below imports are commented as they are not final model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk \n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from joblib import dump, load # used for saving and loading sklearn objects\n",
    "from scipy.sparse import save_npz, load_npz # used for saving and loading sparse matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.b. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO: import other libraries as necessary\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(\"Data\",\"sentiment_train.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(\"Data\",\"sentiment_test.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.c.  Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#excluding not from stop word list as it is important word for sentiment analysis\n",
    "lst_stopwords = lst_stopwords.remove(\"not\")\n",
    "ps = nltk.stem.porter.PorterStemmer()\n",
    "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    processed_data = []\n",
    "    for sentence in data:\n",
    "        #exclude special characters\n",
    "        processed_feature = re.sub(r'\\W', ' ', sentence)\n",
    "        #exclude single characters\n",
    "        processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "        #exclude multiple spaces with single space\n",
    "        processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        processed_feature = processed_feature.lower()\n",
    "        \n",
    "        lst_text = processed_feature.split()\n",
    "        ## remove Stopwords\n",
    "        if lst_stopwords is not None:\n",
    "            lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "        \n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "        \n",
    "        processed_data.append(\" \".join(lst_text))\n",
    "    return processed_data \n",
    "\n",
    "\n",
    "df_train['Sentence_clean'] = clean_data(df_train['Sentence'])\n",
    "df_test['Sentence_clean'] = clean_data(df_test['Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorization_data(data1,data2):\n",
    "        \n",
    "    count_vect = CountVectorizer(ngram_range=(1, 3), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "    X_train_counts = count_vect.fit_transform(data1.astype('U'))\n",
    "    tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "    X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    \n",
    "    X_new_counts = count_vect.transform(data2.astype('U'))\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "    \n",
    "    return X_train_tfidf, X_new_tfidf\n",
    "Processed_features_train, Processed_features_test = vectorization_data(df_train['Sentence_clean'] , df_test['Sentence_clean'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Words_In_Train = []\n",
    "Words_In_Train_0_Polarity = []\n",
    "Words_In_Train_1_Polarity = []\n",
    "row_iter = 0\n",
    "for sentence in df_train['Sentence_clean']:\n",
    "    for word in sentence.split(\" \"):\n",
    "        Words_In_Train.append(word)\n",
    "        if df_train['Polarity'][row_iter] == 0:\n",
    "            Words_In_Train_0_Polarity.append(word)\n",
    "        if df_train['Polarity'][row_iter] == 1:\n",
    "            Words_In_Train_1_Polarity.append(word)\n",
    "    \n",
    "        \n",
    "    row_iter = row_iter+ 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.a. SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** SVM ******************\n",
      "SVM - Best parameters set found on Training Data:\n",
      "\n",
      "{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "SVM - Results on Training Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1213\n",
      "           1       1.00      1.00      1.00      1187\n",
      "\n",
      "    accuracy                           1.00      2400\n",
      "   macro avg       1.00      1.00      1.00      2400\n",
      "weighted avg       1.00      1.00      1.00      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Assumption -- it is okay to report positive comment as negative. but it is not okay to report negative comment as positive\n",
    "#so, False Positives as okay. but False Negatives are not acceptable. so, the modfel should be trained on Recall\n",
    "score = \"recall\"\n",
    "print(\"*********** SVM ******************\")\n",
    "svm_tuned_parameters = [\n",
    "    {\"kernel\": [\"rbf\"], \"gamma\": [1e-3, 1e-4], \"C\": [1, 10, 100, 1000]},\n",
    "    {\"kernel\": [\"linear\"], \"C\": [1, 10, 100, 1000]},\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "svm_grid = GridSearchCV(SVC(), svm_tuned_parameters, scoring= score)\n",
    "svm_grid.fit(Processed_features_train, df_train['Polarity'])\n",
    "\n",
    "print(\"SVM - Best parameters set found on Training Data:\")\n",
    "print()\n",
    "print(svm_grid.best_params_)\n",
    "print()\n",
    "print(\"SVM - Results on Training Data:\")\n",
    "y_true, y_pred = df_train['Polarity'], svm_grid.predict(Processed_features_train)\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.b.  Naive Bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Naive Bias ******************\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Naive Bias - Best parameters set found on Training Data:\n",
      "\n",
      "{'alpha': 0.1}\n",
      "\n",
      "Naive Bias - Results on Train Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1213\n",
      "           1       1.00      1.00      1.00      1187\n",
      "\n",
      "    accuracy                           1.00      2400\n",
      "   macro avg       1.00      1.00      1.00      2400\n",
      "weighted avg       1.00      1.00      1.00      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Naive Bias\n",
    "print(\"************ Naive Bias ******************\")\n",
    "NB_params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0, ],}\n",
    "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=NB_params, n_jobs=-1, cv=5, verbose=5,scoring = score)\n",
    "multinomial_nb_grid.fit(Processed_features_train, df_train['Polarity'])\n",
    "\n",
    "y_true, y_pred = df_train['Polarity'], multinomial_nb_grid.predict(Processed_features_train)\n",
    "print(\"Naive Bias - Best parameters set found on Training Data:\")\n",
    "print()\n",
    "print(multinomial_nb_grid.best_params_)\n",
    "print()\n",
    "print(\"Naive Bias - Results on Train Data:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.c. Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Random Forest ******************\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      " Random Forest  - Best parameters set found on Training Data:\n",
      "\n",
      "{'criterion': 'entropy', 'max_depth': 100, 'n_estimators': 100, 'random_state': 0}\n",
      "\n",
      " Random Forest - Results on Train Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1213\n",
      "           1       1.00      0.98      0.99      1187\n",
      "\n",
      "    accuracy                           0.99      2400\n",
      "   macro avg       0.99      0.99      0.99      2400\n",
      "weighted avg       0.99      0.99      0.99      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"************ Random Forest ******************\")\n",
    "RF_param_grid = {'n_estimators': [ 100,200],'criterion':['gini', 'entropy']#'min_samples_split' : [2, 5, 10, 15, 100],#'min_samples_leaf' : [1, 2, 5, 10] \n",
    "                 ,'max_depth':[50,100,200,500,1000],'random_state':[0]}\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "RF_grid_search = GridSearchCV(estimator = rf, param_grid = RF_param_grid, cv = 3, n_jobs = 4, verbose = 2,scoring = score)\n",
    "RF_grid_search.fit(Processed_features_train, df_train['Polarity'])\n",
    "y_true, y_pred = df_train['Polarity'], RF_grid_search.predict(Processed_features_train)\n",
    "print(\" Random Forest  - Best parameters set found on Training Data:\")\n",
    "print()\n",
    "print(RF_grid_search.best_params_)\n",
    "print()\n",
    "\n",
    "    \n",
    "print(\" Random Forest - Results on Train Data:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Assessing model againts Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"SVM - Results on Test Data:\")\n",
    "y_true, y_pred = df_test['Polarity'], svm_grid.predict(Processed_features_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"print(\"Naive Bias - Results on Test Data:\")\n",
    "y_true, y_pred = df_test['Polarity'], multinomial_nb_grid.predict(Processed_features_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\" Random Forest - Results on Test Data:\")\n",
    "y_true, y_pred = df_test['Polarity'], RF_grid_search.predict(Processed_features_test)\n",
    "print(classification_report(y_true, y_pred))\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Evaluating Results - In english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are optimized for **recall**. because, we dont want to miss any negative comment. so, the recall for the model should be good enough to not miss any negative sentiments.\n",
    "\n",
    "I choose SVM model as by definition, it gives maximum speration between classes. also, the recall & f1 score is highest\n",
    "\n",
    "so, it means, **with 83% probability this model is able to catch the** ***every*** **negative sentiment**. so, i am satisfied from business "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Understanding scope of improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By digging, i figured out **not** is removed by stopwords. so i excluded that from stop words and boosted the performance of model\n",
    "\n",
    "80% of the mismatched data have equal words from 0,1 polarity from the train data and hence the model did not pick up right tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['predict_score'] = svm_grid.predict(Processed_features_train)\n",
    "unmatched_data = df_train[df_train['predict_score'] != df_train['Polarity']]\n",
    "unmatched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In Train data, the junk data resulted in incorrect predictions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['predict_score'] = svm_grid.predict(Processed_features_test)\n",
    "unmatched_data = df_test[df_test['predict_score'] != df_test['Polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_in_train(sentence,Words_In_Train):\n",
    "    count = 0\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in Words_In_Train:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_data['words_in_train'] = [ words_in_train(sentence,Words_In_Train) for sentence in  unmatched_data['Sentence_clean'] ]\n",
    "unmatched_data['words_match_in_0_Polarity'] = [ words_in_train(sentence,Words_In_Train_0_Polarity) for sentence in  unmatched_data['Sentence_clean'] ]\n",
    "unmatched_data['words_match_in_1_Polarity'] = [ words_in_train(sentence,Words_In_Train_1_Polarity) for sentence in  unmatched_data['Sentence_clean'] ]\n",
    "unmatched_data['differntiating_words_ratio'] = unmatched_data['words_match_in_0_Polarity'] /unmatched_data['words_match_in_1_Polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.histogram(unmatched_data['differntiating_words_ratio'] )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.ecdf(unmatched_data['differntiating_words_ratio'] )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80% of the mismatched data have equal words from 0,1 polarity from the train data and hence the model did not pick up right tag"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
